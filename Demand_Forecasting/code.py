# -*- coding: utf-8 -*-
"""Group_No 4_Project_Python_Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PI6V5VPaoB3zKp9RozNkUfbHZlNJSinA

# Marketing Analytics Project

# Group No : 4

### <u>Group Member 1:</u> Divya Rao, 0749348, section 004
### <u>Group Member 3:</u> Mohit Kumar,0749349, section 002

# Data Description:<hr>

The data which we are going to use in our project is downloaded from <a herf ="https://www.kaggle.com/aungpyaeap/supermarket-sales">Kaggle</a>. This data is about supermarket sales, and this data is for three diffrenet brnach over the period of three months between January 2019 to March 2019.

Below is brief description of each columns in our data.



*    Invoice id (Numerical): Computer generated sales slip invoice identification number
*    Branch (Categorical): Branch of supercenter (3 branches are available identified by   A, B and C).
*   City (Categorical): Location of supercenters
*   Customer type (Categorical): Type of customers, recorded by Members for customers using member card and Normal for without member card.

*   Gender (Categorical): Gender type of customer
*   Product line: General item categorization groups - Electronic accessories, Fashion accessories, Food and beverages, Health and beauty, Home and lifestyle, Sports and travel
*   Unit price (Numerical): Price of each product in $
*   Quantity (Numerical): Number of products purchased by customer
*   Tax (Numerical): 5% tax fee for customer buying
*   Total (Numerical): Total price including tax
*   Date (Numerical): Date of purchase (Record available from January 2019 to March 2019)
*   Time (Numerical): Purchase time (10am to 9pm)
*   Payment (Categorical): Payment used by customer for purchase (3 methods are available â€“ Cash, Credit card and Ewallet)
*   COGS (Numerical): Cost of goods sold
*   Gross margin percentage (Numerical): Gross margin percentage
*   Gross income (Numerical): Gross income generated from particular product sold
*   Rating (Numerical): Customer statification rating on their overall shopping experience (On a scale of 1 to 10)

# Basic Understanding of Data
"""

# Commented out IPython magic to ensure Python compatibility.
# Imporing required libraries for project

import pandas as pd # for data manipluation
import numpy as np  # for data calculations and statistical measurement
import matplotlib.pyplot as plt #for ineractive visualization charts
import seaborn as sns #for ineractive visualization charts
# %matplotlib inline 
import warnings  # For warnings
warnings.filterwarnings("ignore") # To ignore unwanted warnings


import itertools
import statsmodels.api as sm # for applying stats model to do forecasting

# Uploading Data from local system to google colab
#from google.colab import files
#uploaded = files.upload()

# Reading Dataset and loading into variable
import io
data = pd.read_csv('supermarket-sales - Sheet1.xls')

# looking at first few rows of data
pd.set_option('display.max_columns', None)

data.head()

# structer Of data
data.shape

"""**There are a total 1000 rows and 17 columns in our data.**"""

# Checking data type and null values for each column
data.info()

"""**From the above output we can see that most of the column is having proper data type but date column is not in proper format. we will change it to date time before starting our analysis. by combing date and time column and changing it's data type to datetime.**"""

# Checking for null values
data.isnull().sum()

"""**We can see that there are no missing values in our dataset, hence we do not need to perform any data handling process for our data.**"""

# Checking five Point Summary For All Numerical Columns
data.describe()

"""**Form the five point summary of numerical columns we can notice that highest individual total sale noted was 1042.650 and highest individual quantity sold was 10. Overall Average Customer rating is 6.97.**"""

# Exaploring unqie values of Branch column
data['Branch'].value_counts()

# Exaploring unqie values of City column
data['City'].value_counts()

"""**After printing unique values and their counts for Branch and City column it seems that both are same meaning that if you observe then city column has 340 values of  city 'Yangon' and branch column has same 340 values of branch 'A'. The same thing you can notice for other two cities and branches. So, If we use either of the column for analysis it will be same.**"""

# Exaploring unqie values of Gender column
data['Gender'].value_counts()

# Exaploring unqie values of Customer Type column
data['Customer type'].value_counts()

"""**There are 501 values for member customer type and 499 values for normal cutomer type.**"""

# Exaploring unqie values of Product line column
data['Product line'].value_counts()

"""**There are a total 5 unique values for product line column and the highest is Fashion and accessories product values are in product line column.**"""

# Exaploring unqie values of Payment column
data['Payment'].value_counts()

"""**There are three unique values for payment and highest payment mode used is Ewallet.**"""

# Converting our date and time column to datetime
data['Date'] = pd.to_datetime(data['Date'])
data.head(2)

# Creating new column by combing date and time column
data['DateTime'] = pd.to_datetime(data.Date.astype('str')+' '+data.Time.astype('str'))

# Cross checking our changes 
print(data['DateTime'].dtype)

#Pritinng data after making changes
data.head()

data.shape

#Creating a column for Hours
data['Hour'] = pd.to_datetime(data['Time'], format ='%H:%M').dt.hour
data.head()

"""**We can see that date column datatype is changed to datetime and also newly created column datetime is being added to our main data.**

# **Sales Forecast For City Yangon**
"""

#Getting data only for city yangon
yangon = data.loc[data['City'] == 'Yangon']

#printing head of yangon data
yangon.head()

# printing columns for yangon data
yangon.columns

# Removing all columns apart from date and total sales 
r_col = ['Invoice ID', 'Branch', 'City', 'Customer type', 'Gender',
       'Product line', 'Unit price', 'Quantity', 'Tax 5%',
       'Time', 'Payment', 'cogs', 'gross margin percentage', 'gross income',
       'Rating', 'DateTime', 'Hour']


yangon.drop(r_col, axis =1 , inplace=True)

# looking at data after dropping columns
yangon

# changing order of column
yangon = yangon[["Date","Total"]]
yangon

# sorting value for by date and seting index
yangon = yangon.sort_values('Date')

yangon

yangon.set_index('Date', inplace=True)

yangon

yangon.columns

#Resampling the data using Calender Day Frequency and taking their average
#yangon = yangon['Total'].resample('D').mean()
# D = Calendar Day frequency

yangon.head(4)

yangon.shape

yangon.plot(figsize=(15,6),legend=True)
plt.ylabel("Sales",fontsize=18)
plt.xlabel("Date",fontsize=18)
plt.title("Date Vs Sales",fontsize=20)
plt.show()

"""# **Testing for Stationarity**"""

from statsmodels.tsa.stattools import adfuller

test_result = adfuller(yangon['Total'])

test_result

"""#### **We're using Dickey Fuller Test here to test for stationarity. The Dickey Fuller Test gives us 5 values, namely - ADF Test Statitic, p-value, \#Lags used & Number of Observations used. However, our main focus here is on the p-value.**

#### ** We are assuming our H0 as "Our data is not stationary" and H1 as "Our data is stationary". **

#### From the above 5 values, we see that our p-value is 2.034195712964938e-30 which is 0.000000000000000000000000000002034195712964938 in real numbers. Therefore, we can see that our p-value is less than 0.05 and hence we cannot accept our null hypothesis and that the data is stationary.

## **Resampling the data**
"""

#Resampling the data using Calender Day Frequency and taking their average
yangon = yangon['Total'].resample('D').mean()
yangon.head()
# D = Calendar Day frequency

from pylab import rcParams as rc

rc['figure.figsize'] = 10, 14

decomposition = sm.tsa.seasonal_decompose(yangon,model='additive', freq=30)

#Finding trend,seasonal,observed and residual values

fig = decomposition.plot()
plt.show()
# y(t) = Level + Trend + Seasonality + Noise --> Additive

"""# **Forecasting with ARIMA**"""

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

fig = sm.graphics.tsa.plot_acf(yangon, lags=40)

fig = sm.graphics.tsa.plot_pacf(yangon, lags=40)

p=d=q=range(0,2)

p,d,q

pdq = list(itertools.product(p,d,q))

pdq

seasonal_pdq = [(x[0],x[1],x[2], 12) for x in pdq]

seasonal_pdq

for param in pdq:
    for param_seasonal in seasonal_pdq:
        try:
            mod = sm.tsa.statespace.SARIMAX(yangon,order = param, seasonal_order = param_seasonal ,
                                            enforce_stationarity= False , enforce_invertibility= False )
            results = mod.fit()
            
            print('ARIMA{} x {} 12 -- AIC : {}'.format(param, param_seasonal, results.aic))
            
        except:
             continue

mod = sm.tsa.statespace.SARIMAX(yangon,
                               order=(1,0,1),
                               seasonal_order= (1,1,1,12),
                               enforce_stationarity = False,
                               enforce_invertibility=False)

results = mod.fit()

print(results.summary().tables[1])

results.plot_diagnostics(figsize=(16,8))
plt.show()

pred = results.get_prediction(start = pd.to_datetime('2019-01-01'), dynamic = False)
pred_ci = pred.conf_int()

ax = yangon['2019':].plot(label= 'observed')

pred.predicted_mean.plot(ax = ax, label = 'One step ahead Forecast',
                        alpha = 7, figsize= (14,7))

ax.fill_between(pred_ci.index,
               pred_ci.iloc[:,0],
               pred_ci.iloc[:,1],color = 'k', alpha= 0.2)

ax.set_xlabel('Date')
ax.set_ylabel('Total')
plt.legend()

plt.show()

yangon_forecasted = pred.predicted_mean
yangon_truth = yangon['2019-01-01':]
mse = ((yangon_forecasted - yangon_truth) ** 2).mean()

print('MSE of forecast :{}'.format(round(mse,2)))

pred_uc = results.get_forecast(steps = 10)
pred_ci = pred_uc.conf_int()

ax = yangon.plot(label='observed', figsize=(10,8))
pred_uc.predicted_mean.plot(ax=ax, label='Forecast')
ax.fill_between(pred_ci.index,
               pred_ci.iloc[:,0],
               pred_ci.iloc[:,1],color='k',alpha=0.6)
ax.set_xlabel('Date')
ax.set_ylabel('Total')

plt.legend()
plt.show()

"""#### We have used 10 step forecasting for preidcting the sales for the next 10 days."""

pred_uc = results.get_forecast(steps = 60)
pred_ci = pred_uc.conf_int()

ax = yangon.plot(label='observed', figsize=(10,8))
pred_uc.predicted_mean.plot(ax=ax, label='Forecast')
ax.fill_between(pred_ci.index,
               pred_ci.iloc[:,0],
               pred_ci.iloc[:,1],color='k',alpha=0.6)
ax.set_xlabel('Date')
ax.set_ylabel('Total')

plt.legend()
plt.show()

"""#### In the above graph, we have plotted for the next 50 days or we can also say that we are predicting for the next 2 months."""

